{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Discriminant Analysis for Spam Email Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 172 features, but StandardScaler is expecting 5000 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m     18\u001b[0m train_X \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(train_X)\n\u001b[1;32m---> 19\u001b[0m test_X \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(test_X)\n",
      "File \u001b[1;32md:\\Python\\Machine-Learing-Projects\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32md:\\Python\\Machine-Learing-Projects\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    989\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    991\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m--> 992\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    993\u001b[0m     X,\n\u001b[0;32m    994\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    995\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    996\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    997\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    998\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    999\u001b[0m )\n\u001b[0;32m   1001\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m   1002\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32md:\\Python\\Machine-Learing-Projects\\.venv\\Lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    590\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\Python\\Machine-Learing-Projects\\.venv\\Lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 172 features, but StandardScaler is expecting 5000 features as input."
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "email_data = pd.read_csv('./dataset/emails.csv', header=0)\n",
    "\n",
    "# Remove the First Column\n",
    "email_data = email_data.drop(email_data.columns[0], axis=1)\n",
    "\n",
    "# Training Set\n",
    "train_X = email_data.iloc[0:5000, 0:-1].values.T.astype(np.float64)\n",
    "train_Y = email_data.iloc[0:5000:, -1].values.astype(np.float64)\n",
    "\n",
    "# Test Set\n",
    "test_X = email_data.iloc[5000:, 0:-1].values.T.astype(np.float64)\n",
    "test_Y = email_data.iloc[5000:, -1].values.astype(np.float64)\n",
    "\n",
    "# Standardize the Features for Both Training Set and Test Set\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "data_dim, train_data_size = train_X.shape\n",
    "test_data_size = test_X.shape[1]\n",
    "# Swap 0s with 1s and 1s with 0s in Output Set for Summation of Indicator Functions\n",
    "swapped_train_Y = np.where(train_Y==0, 1, 0) \n",
    "swapped_test_Y = np.where(test_Y==0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Parameters of The GDA Model\n",
    "prior_prob = 1 / train_data_size * np.sum(train_Y) #! φ = 1/n * ΣI(y^(i)=1)\n",
    "mean_y0 = train_X @ swapped_train_Y / np.sum(swapped_train_Y) #! μ0 = ΣI(y^(i)=0) * x^(i) / ΣI(y^(i)=0)\n",
    "mean_y1 = train_X @ train_Y / np.sum(train_Y) #! μ0 = ΣI(y^(i)=1) * x^(i) / ΣI(y^(i)=1)\n",
    "\n",
    "# Reshape means to be column vectors\n",
    "mean_y0 = mean_y0.reshape(-1, 1)\n",
    "mean_y1 = mean_y1.reshape(-1, 1)\n",
    "\n",
    "# Subtract Means from Input X Based on Corresponding Y\n",
    "X_mu = train_X.copy()\n",
    "X_mu[:, train_Y == 0] -= mean_y0 #! Σ = 1/n * Σ((x^(i) - μy^(i)) * (x^(i) - μy^(i))^T)\n",
    "X_mu[:, train_Y == 1] -= mean_y1\n",
    "\n",
    "covariance_matrix = (X_mu @ X_mu.T) / train_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions for Two Classes\n",
    "prob_y0 = 1- prior_prob\n",
    "prob_y1 = prior_prob\n",
    "\n",
    "# Calculate the Inverse and Determinant of the Covariance Matrix\n",
    "covariance_matrix_inv = np.linalg.inv(covariance_matrix)\n",
    "covariance_matrix_det = np.linalg.det(covariance_matrix)\n",
    "\n",
    "# Constant Factor for the Multivariate Guassian Distribution\n",
    "const_factor = 1 / ((2 * np.pi) ** (data_dim / 2) * np.sqrt(covariance_matrix_det))\n",
    "\n",
    "# Subtract the Means from the Test Set\n",
    "diff_y0 = test_X - mean_y0\n",
    "diff_y1 = test_X - mean_y1\n",
    "\n",
    "# Compute Exponential Part\n",
    "exp_arg0 = np.sum((diff_y0.T @ covariance_matrix_inv) * diff_y0.T, axis=1)\n",
    "exp_arg1 = np.sum((diff_y1.T @ covariance_matrix_inv) * diff_y1.T, axis=1)\n",
    "\n",
    "# Compute the Probability Densities\n",
    "p_x_given_y0 = const_factor * np.exp(-0.5 * exp_arg0) #! p(x|y=0) = (1 / (2π)^(n/2) * |Σ|^(1/2)) * exp(-1/2 * (x - μ0)^T * Σ^-1 * (x - μ0))\n",
    "p_x_given_y1 = const_factor * np.exp(-0.5 * exp_arg1) #! p(x|y=1) = (1 / (2π)^(n/2) * |Σ|^(1/2)) * exp(-1/2 * (x - μ1)^T * Σ^-1 * (x - μ1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x_given_y0, p_x_given_y1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
